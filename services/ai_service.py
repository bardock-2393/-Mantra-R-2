"""
AI Service Module - Round 2 Performance Optimized
Handles Gemma 3 model interactions and analysis functionality
Enhanced for sub-1000ms latency with caching and performance monitoring
"""

import asyncio
import torch
import hashlib
import time
import numpy as np
from transformers import AutoProcessor, Gemma3ForConditionalGeneration, pipeline
from PIL import Image
import cv2
import os
import tempfile
import shutil
from typing import Tuple, Optional, List
from config import Config
from analysis_templates import generate_analysis_prompt
from services.performance_service import measure_latency, cache_manager

# Import WebSocket service for real-time updates (avoiding circular import)
def get_websocket_service():
    """Get WebSocket service with delayed import to avoid circular dependency"""
    try:
        from services.websocket_service import get_websocket_service
        return get_websocket_service()
    except ImportError:
        return None

# Disable compilation to save disk space
os.environ['TORCHDYNAMO_DISABLE'] = '1'
os.environ['TORCH_COMPILE_DISABLE'] = '1'

# Initialize Gemma 3 model - Optimized for A100 80GB with space constraints
model_id = "google/gemma-3-12b-it"  # Use 12B to avoid disk space issues
device = "cuda" if torch.cuda.is_available() else "cpu"

# Global variables for model and processor (lazy loading)
model = None
processor = None

def cleanup_cache():
    """Clean up cache and temporary files to free disk space"""
    try:
        # Clean up /tmp directory
        for item in os.listdir('/tmp'):
            if item.startswith('torch') or item.startswith('torchinductor'):
                try:
                    path = os.path.join('/tmp', item)
                    if os.path.isdir(path):
                        shutil.rmtree(path)
                    else:
                        os.remove(path)
                except:
                    pass
        print("‚úÖ Cleaned up temporary files")
    except Exception as e:
        print(f"Cache cleanup warning: {e}")

def initialize_model():
    """Initialize Gemma 3 model and processor - A100 Optimized with space management"""
    global model, processor
    if model is None:
        print("Loading Gemma 3 12B model for A100...")
        
        # Clean up cache first
        cleanup_cache()
        
        try:
            # 80GB GPU optimized settings for maximum performance
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.set_float32_matmul_precision('high')
            # Maximum speed optimizations for 80GB GPU
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            # 80GB GPU specific optimizations
            if Config.HIGH_MEMORY_MODE:
                torch.cuda.set_per_process_memory_fraction(0.8)  # Use 80% of 80GB
                print(f"üöÄ 80GB GPU Mode: Using up to {Config.GPU_MEMORY_GB * 0.8:.1f}GB GPU memory")
            
            # Use low_cpu_mem_usage to reduce memory pressure
            model = Gemma3ForConditionalGeneration.from_pretrained(
                model_id, 
                device_map="auto",
                torch_dtype=torch.bfloat16,  # A100 supports bfloat16 natively
                trust_remote_code=True,
                low_cpu_mem_usage=True
            ).eval()
            
            processor = AutoProcessor.from_pretrained(model_id)
            
            # Skip compilation to save disk space
            print("‚úÖ Gemma 3 12B model loaded successfully on A100 (compilation disabled)")
            if torch.cuda.is_available():
                print(f"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
                print(f"üìä GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.1f}GB")
            
        except Exception as e:
            print(f"Error loading Gemma 3 12B model: {e}")
            raise e

@measure_latency("frame_extraction")
def extract_video_frames(video_path, num_frames=None) -> Tuple[List[Image.Image], List[float], float]:
    """Extract frames from video for analysis - A100 Optimized with Smart Sampling + Round 2 Caching"""
    
    # Check cache first for massive speed improvement
    cache_key = cache_manager.get_cache_key(
        video_path, 
        "frames", 
        num_frames=num_frames,
        file_mtime=os.path.getmtime(video_path)
    )
    
    cached_result = cache_manager.get(cache_key)
    if cached_result:
        print(f"üöÄ CACHE HIT: Frame extraction from cache in <0.01s")
        # Convert cached data back to PIL Images
        frames = [Image.fromarray(frame_data) for frame_data in cached_result['frames']]
        return frames, cached_result['timestamps'], cached_result['duration']
    
    import random
    
    cap = cv2.VideoCapture(video_path)
    frames = []
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = frame_count / fps if fps > 0 else 0
    
    # Auto-determine frame count based on video length - 80GB GPU Optimized
    if num_frames is None:
        if duration >= Config.LONG_VIDEO_THRESHOLD:  # 1+ hour videos
            num_frames = Config.MAX_FRAMES_LONG_VIDEO
            print(f"üé¨ Long video detected ({duration/60:.1f} minutes) - using 80GB GPU optimized sampling with {num_frames} frames")
        else:
            num_frames = Config.MAX_FRAMES_SHORT_VIDEO
            print(f"üé¨ Short video ({duration/60:.1f} minutes) - using 80GB GPU optimized sampling with {num_frames} frames")
    
    # 80GB GPU can handle more aggressive processing
    if Config.HIGH_MEMORY_MODE and duration < 300:  # Under 5 minutes
        num_frames = min(num_frames + 2, 8)  # Boost frames for short videos
        print(f"üöÄ 80GB GPU Boost: Increased to {num_frames} frames for better quality")
    
    # Smart sampling strategy for different video lengths
    if duration >= Config.LONG_VIDEO_THRESHOLD:  # Long videos (1+ hours)
        # Strategic frame selection for long videos
        key_points = [
            0,  # Start
            frame_count // 6,  # ~17%
            frame_count // 3,  # ~33%
            frame_count // 2,  # 50% (middle)
            2 * frame_count // 3,  # ~67%
            5 * frame_count // 6,  # ~83%
            frame_count - 1  # End
        ]
        
        # Add random samples from different segments for variety
        if num_frames > len(key_points):
            segment_size = frame_count // num_frames
            for i in range(len(key_points), num_frames):
                random_frame = i * segment_size + random.randint(0, min(segment_size//2, frame_count//20))
                key_points.append(min(random_frame, frame_count-1))
        
        frame_indices = sorted(list(set(key_points)))[:num_frames]
        print(f"üìä Smart sampling: {len(frame_indices)} strategic frames from {frame_count} total frames")
    else:
        # Regular distributed sampling for shorter videos
        frame_indices = [int(i * frame_count / num_frames) for i in range(num_frames)]
        print(f"üìä Regular sampling: {len(frame_indices)} evenly distributed frames")
    
    timestamps = []
    for i, frame_idx in enumerate(frame_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            # Optimized resize for A100 - use faster interpolation for long videos
            interpolation = cv2.INTER_NEAREST if duration >= Config.LONG_VIDEO_THRESHOLD else cv2.INTER_LANCZOS4
            frame_resized = cv2.resize(frame, (896, 896), interpolation=interpolation)
            # Convert BGR to RGB
            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
            # Convert to PIL Image
            pil_image = Image.fromarray(frame_rgb)
            frames.append(pil_image)
            # Calculate timestamp
            timestamp = frame_idx / fps if fps > 0 else 0
            timestamps.append(timestamp)
    
    cap.release()
    
    # Cache the extracted frames for future requests (massive performance boost)
    try:
        # Convert PIL Images to numpy arrays for caching
        cached_frames = [np.array(frame) for frame in frames]
        cache_data = {
            'frames': cached_frames,
            'timestamps': timestamps,
            'duration': duration
        }
        cache_manager.set(cache_key, cache_data, ttl=7200)  # Cache for 2 hours
        print(f"üíæ CACHED: Frame extraction results for future requests")
    except Exception as e:
        print(f"‚ö†Ô∏è Cache storage failed: {e}")
    
    # Memory optimization for long videos - 80GB GPU optimized
    if duration >= Config.LONG_VIDEO_THRESHOLD:
        if Config.HIGH_MEMORY_MODE:
            # With 80GB GPU, we can be less aggressive with cache clearing
            if torch.cuda.memory_allocated() > (Config.GPU_MEMORY_GB * 0.6 * 1e9):
                torch.cuda.empty_cache()
                print("üóÑÔ∏è 80GB GPU: Smart cache management - cleared cache")
        else:
            torch.cuda.empty_cache()  # Clear GPU memory after processing
    
    return frames, timestamps, duration

@measure_latency("video_analysis")
async def analyze_video_with_gemini(video_path, analysis_type, user_focus, session_id=None):
    """Analyze video using Gemma 3 model with real-time WebSocket updates - Round 2 Optimized"""
    import time
    start_time = time.time()
    print(f"üé¨ Starting video analysis at {time.strftime('%H:%M:%S')}")
    
    # Check cache for complete analysis first
    cache_key = cache_manager.get_cache_key(
        video_path, 
        "analysis", 
        analysis_type=analysis_type,
        user_focus=user_focus,
        file_mtime=os.path.getmtime(video_path)
    )
    
    cached_analysis = cache_manager.get(cache_key)
    if cached_analysis:
        print(f"üöÄ CACHE HIT: Complete analysis from cache in <0.01s")
        return cached_analysis['result']
    
    # Get WebSocket service for real-time updates
    ws_service = get_websocket_service()
    
    try:
        # Generate analysis prompt based on type and user focus
        analysis_prompt = generate_analysis_prompt(analysis_type, user_focus)
        
        # Emit progress: Starting analysis
        if ws_service and session_id:
            ws_service.emit_progress(session_id, 'initialization', 10, 'üöÄ Starting video analysis...')
        
        # Clean up before analysis
        cleanup_cache()
        
        # Extract frames from video with smart sampling
        frame_start = time.time()
        print('üé¨ Extracting frames from video...')
        
        if ws_service and session_id:
            ws_service.emit_progress(session_id, 'frame_extraction', 25, 'üé¨ Extracting frames from video...')
        
        frames, timestamps, duration = extract_video_frames(video_path)  # Auto-detects long videos
        frame_time = time.time() - frame_start
        
        if not frames:
            raise ValueError("No frames could be extracted from the video")
        
        print(f'‚úÖ Extracted {len(frames)} frames in {frame_time:.2f}s (video duration: {duration:.2f}s)')
        
        # Emit progress: Frames extracted
        if ws_service and session_id:
            ws_service.emit_progress(session_id, 'frame_extraction', 50, f'‚úÖ Extracted {len(frames)} frames ({duration/60:.1f} min video)')
        
        # Adaptive system prompt based on video length
        is_long_video = duration >= Config.LONG_VIDEO_THRESHOLD
        
        # Notify about long video detection
        if ws_service and session_id and is_long_video:
            ws_service.emit_long_video_detected(session_id, duration/60)
        video_type = "long-form" if is_long_video else "short-form"
        
        if is_long_video:
            # Condensed prompt for long videos (faster processing)
            agent_system_prompt = f"""
You are an **AI video analysis agent** optimized for {video_type} content analysis. Provide **concise, precise analysis** focusing on key elements.

## ANALYSIS PROTOCOL FOR LONG VIDEOS ({duration/60:.1f} minutes)

### Key Requirements:
- **Concise Summaries**: Focus on major themes and key moments
- **Strategic Timestamps**: Highlight critical transitions and events
- **Pattern Recognition**: Identify recurring elements across the video
- **Efficient Structure**: Use clear headings and bullet points

### Analysis Focus:
- **Major Events**: Key moments and transitions
- **Visual Themes**: Consistent elements throughout
- **Technical Quality**: Overall production assessment
- **Narrative Flow**: Story progression and pacing

**Video Duration**: {duration:.1f}s ({duration/60:.1f} minutes)
**Frames Analyzed**: {len(frames)} strategic samples at: {[f'{t:.1f}s' for t in timestamps]}

Provide **concise, actionable insights** suitable for real-time analysis.
"""
        else:
            # Detailed prompt for shorter videos
            agent_system_prompt = f"""
You are an **exceptional AI video analysis agent** providing **comprehensive analysis** for {video_type} content.

## DETAILED ANALYSIS PROTOCOL

### Analysis Standards:
- **Maximum Precision**: Exact timestamps and measurements
- **Comprehensive Coverage**: Analyze all significant aspects
- **Detailed Descriptions**: Vivid, descriptive language
- **Evidence-Based**: Support observations with visual evidence

### Focus Areas:
- **Temporal Precision**: Exact timestamps for events
- **Visual Details**: Colors, lighting, composition
- **Behavioral Analysis**: Actions and interactions
- **Technical Assessment**: Quality and production values

**Video Duration**: {duration:.1f}s
**Frames Analyzed**: {len(frames)} at timestamps: {[f'{t:.1f}s' for t in timestamps]}

Provide **detailed, comprehensive analysis** for optimal user experience.
"""
        
        # Analyze the first frame to get initial insights, then analyze sequence
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": agent_system_prompt}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": frames[0]},
                    {"type": "text", "text": f"Analyze this video sequence. {analysis_prompt}"}
                ]
            }
        ]
        
        # Adaptive frame processing based on video length
        if is_long_video:
            # For long videos: Use more frames but process efficiently 
            max_additional_frames = min(len(frames) - 1, 3)  # Up to 3 additional frames
            frame_step = max(1, (len(frames) - 1) // max_additional_frames) if max_additional_frames > 0 else 1
            selected_frames = frames[1::frame_step][:max_additional_frames]
            selected_timestamps = timestamps[1::frame_step][:max_additional_frames]
            
            for i, (frame, timestamp) in enumerate(zip(selected_frames, selected_timestamps), 1):
                messages[-1]["content"].append({"type": "image", "image": frame})
                messages[-1]["content"].append({"type": "text", "text": f"Key frame at {timestamp:.1f}s:"})
            
            print(f"üìä Processing {len(selected_frames)} key frames for long video analysis")
        else:
            # For short videos: Use minimal frames for speed
            for i, frame in enumerate(frames[1:1], 1):  # Use 0 additional frames for max speed
                messages[-1]["content"].append({"type": "image", "image": frame})
                messages[-1]["content"].append({"type": "text", "text": f"Frame at {timestamps[i]:.2f}s:"})
            
            print(f"üìä Processing {len(frames[1:1])} additional frames for short video analysis")
        
        inputs = processor.apply_chat_template(
            messages, 
            add_generation_prompt=True, 
            tokenize=True,
            return_dict=True, 
            return_tensors="pt"
        ).to(model.device, dtype=torch.bfloat16)
        
        input_len = inputs["input_ids"].shape[-1]
        
        # Emit progress: AI thinking
        if ws_service and session_id:
            ws_service.emit_ai_thinking(session_id, 'ai_inference')
            ws_service.emit_progress(session_id, 'ai_inference', 75, 'üß† Gemma 3 AI analyzing video content...')
        
        # EXTREME SPEED optimization for sub-1000ms target
        inference_start = time.time()
        print('üß† Starting ULTRA-FAST AI inference...')
        with torch.inference_mode():
            generation = model.generate(
                **inputs, 
                max_new_tokens=Config.MAX_OUTPUT_TOKENS,  # Minimal tokens
                temperature=Config.TEMPERATURE,  # Greedy (fastest)
                top_k=Config.TOP_K,  # Greedy selection
                do_sample=False,  # Greedy decoding (fastest)
                use_cache=True,  # Enable KV cache for speed
                pad_token_id=processor.tokenizer.eos_token_id,
                # Additional speed optimizations
                early_stopping=True,
                num_beams=1,  # No beam search for speed
                output_attentions=False,
                output_hidden_states=False,
                return_dict_in_generate=False
            )
            generation = generation[0][input_len:]
        
        inference_time = time.time() - inference_start
        print(f'‚úÖ AI inference completed in {inference_time:.2f}s')
        
        response_text = processor.decode(generation, skip_special_tokens=True)
        
        # Calculate total time and add timing info to response
        total_time = time.time() - start_time
        print(f'üèÅ Total analysis completed in {total_time:.2f}s')
        
        # Add enhanced timing information for long videos
        video_info = f"({duration/60:.1f} min, {len(frames)} frames)" if is_long_video else f"({duration:.1f}s, {len(frames)} frames)"
        timing_info = f"""

---
‚è±Ô∏è **Performance Report:**
- **Video**: {video_info}
- **Frame Extraction**: {frame_time:.2f}s
- **AI Inference**: {inference_time:.2f}s  
- **Total Time**: {total_time:.2f}s
- **Speed**: {len(frames)/total_time:.1f} frames/sec
- **Mode**: {'Long Video (Smart Sampling)' if is_long_video else 'Short Video (Full Analysis)'} 
- **GPU**: A100 Optimized ‚ö°
"""
        
        # Emit completion with results
        if ws_service and session_id:
            timing_info_dict = {
                'frame_time': frame_time,
                'inference_time': inference_time,
                'total_time': total_time,
                'speed': len(frames)/total_time,
                'mode': 'Long Video (Smart Sampling)' if is_long_video else 'Short Video (Full Analysis)'
            }
            ws_service.emit_analysis_complete(session_id, response_text + timing_info, timing_info_dict)
            ws_service.emit_progress(session_id, 'complete', 100, f'‚úÖ Analysis complete in {total_time:.2f}s!')
        
        # Cache the complete analysis result for future requests
        final_result = response_text + timing_info
        try:
            cache_data = {'result': final_result}
            cache_manager.set(cache_key, cache_data, ttl=7200)  # Cache for 2 hours
            print(f"üíæ CACHED: Complete analysis results for future requests")
        except Exception as e:
            print(f"‚ö†Ô∏è Analysis cache storage failed: {e}")
        
        # Enhanced cleanup for long videos
        cleanup_cache()
        if is_long_video:
            torch.cuda.empty_cache()  # Additional GPU memory cleanup for long videos
            torch.cuda.synchronize()  # Ensure all GPU operations complete
        
        return final_result
        
    except Exception as e:
        print(f"Gemma 3 analysis error: {e}")
        if ws_service and session_id:
            ws_service.emit_error(session_id, str(e), 'analysis')
        cleanup_cache()  # Clean up on error too
        return f"Error analyzing video: {str(e)}"

@measure_latency("chat_response")
def generate_chat_response(analysis_result, analysis_type, user_focus, message, chat_history, session_id=None):
    """Generate contextual AI response with real-time WebSocket updates - Round 2 Optimized"""
    import time
    start_time = time.time()
    print(f"üí¨ Starting chat response at {time.strftime('%H:%M:%S')}")
    
    # Check cache for chat response
    cache_key = cache_manager.get_cache_key(
        str(hash(analysis_result)), 
        "chat", 
        message=message,
        chat_length=len(chat_history)
    )
    
    cached_response = cache_manager.get(cache_key)
    if cached_response:
        print(f"üöÄ CACHE HIT: Chat response from cache in <0.01s")
        return cached_response['response']
    
    # Get WebSocket service for real-time updates
    ws_service = get_websocket_service()
    
    try:
        # Clean up before chat
        cleanup_cache()
        
        # Enhanced agentic conversation prompt with advanced capabilities
        context_prompt = f"""
You are an advanced AI video analysis agent with comprehensive understanding capabilities. You are engaging in a multi-turn conversation about a video that has been analyzed.

## AGENT CONVERSATION PROTOCOL

### Current Context:
- Analysis Type: {analysis_type.replace('_', ' ').title()}
- Original Analysis Focus: {user_focus}
- User Question: "{message}"
- Conversation History: Available for context awareness

### Video Analysis Context:
            {analysis_result}
            
### Agent Capabilities:
- Autonomous Analysis: Provide comprehensive insights beyond the immediate question
- Multi-Modal Understanding: Reference visual, audio, temporal, and spatial elements
- Context Awareness: Adapt responses based on conversation history and user intent
- Proactive Insights: Offer additional relevant information and observations
- Comprehensive Reporting: Provide detailed, structured responses
- Adaptive Focus: Adjust response depth based on question complexity

### Response Quality Standards:
1. **Precision & Accuracy**: Provide exact, verifiable information with specific timestamps
2. **Comprehensive Coverage**: Address all aspects of the question thoroughly
3. **Evidence-Based**: Support every claim with specific evidence from the analysis
4. **Clear Structure**: Use logical organization with clear headings and bullet points
5. **Professional Tone**: Maintain engaging yet professional communication
6. **Proactive Insights**: Offer additional relevant observations beyond the direct question
7. **Visual Clarity**: Use formatting to enhance readability (bold, italics, lists)
8. **Contextual Awareness**: Reference previous conversation context when relevant

### Response Format Guidelines:
- **Start with a direct answer** to the user's question
- **Use clear headings** for different sections (e.g., "**Key Findings:**", "**Timeline:**", "**Additional Insights:**")
- **Include specific timestamps** when discussing events (e.g., "At **00:15-00:17**")
- **Use bullet points** for lists and multiple items
- **Bold important information** for emphasis
- **Provide quantitative data** when available (durations, counts, measurements)
- **Include relevant context** that enhances understanding
- **End with actionable insights** or additional observations when relevant

### Specialized Response Areas:
- **Safety Analysis**: Focus on specific safety concerns, violations, and recommendations
- **Timeline Events**: Provide chronological details with precise timestamps
- **Pattern Recognition**: Highlight recurring behaviors, trends, and anomalies
- **Performance Assessment**: Discuss quality, efficiency, and optimization opportunities
- **Creative Elements**: Analyze artistic, aesthetic, and creative aspects
- **Technical Details**: Provide technical specifications and quality assessments
- **Behavioral Insights**: Analyze human behavior, interactions, and social dynamics
- **Environmental Factors**: Consider context, setting, and environmental conditions

### Quality Enhancement Techniques:
- **Quantify responses**: Use specific numbers, durations, and measurements
- **Cross-reference information**: Connect related details across different sections
- **Provide context**: Explain why certain details are significant
- **Use descriptive language**: Make responses vivid and engaging
- **Structure complex information**: Break down complex topics into digestible sections
- **Highlight patterns**: Identify and explain recurring themes or behaviors
- **Offer insights**: Provide analysis beyond simple description

Your mission is to provide **exceptional quality responses** that demonstrate deep understanding of the video content, offer precise information with timestamps, and deliver insights that exceed user expectations. Every response should be comprehensive, well-structured, and highly informative.
"""
        
        # Include conversation history for context awareness
        conversation_context = ""
        if len(chat_history) > 2:  # More than just current message
            recent_messages = chat_history[-6:]  # Last 6 messages for context
            conversation_context = "\n\n### Recent Conversation Context:\n"
            for msg in recent_messages:
                if 'user' in msg:
                    conversation_context += f"User: {msg['user']}\n"
                elif 'ai' in msg:
                    conversation_context += f"Agent: {msg['ai'][:200]}...\n"  # Truncate for context
        
        enhanced_context_prompt = context_prompt + conversation_context
        
        # Create messages for Gemma 3
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": enhanced_context_prompt}]
            },
            {
                "role": "user",
                "content": [{"type": "text", "text": message}]
            }
        ]
        
        inputs = processor.apply_chat_template(
            messages, 
            add_generation_prompt=True, 
            tokenize=True,
            return_dict=True, 
            return_tensors="pt"
        ).to(model.device)
        
        input_len = inputs["input_ids"].shape[-1]
        
        with torch.inference_mode():
            generation = model.generate(
                **inputs, 
                max_new_tokens=Config.CHAT_MAX_TOKENS,  # Minimal tokens
                temperature=Config.CHAT_TEMPERATURE,  # Greedy
                top_k=Config.TOP_K,  # Greedy selection
                do_sample=False,  # Greedy decoding (fastest)
                use_cache=True,  # KV cache
                pad_token_id=processor.tokenizer.eos_token_id,
                # Speed optimizations for chat
                early_stopping=True,
                num_beams=1,
                output_attentions=False,
                output_hidden_states=False,
                return_dict_in_generate=False
            )
            generation = generation[0][input_len:]
        
        response_text = processor.decode(generation, skip_special_tokens=True)
        
        # Calculate chat response time and add to response
        chat_time = time.time() - start_time
        print(f'üí¨ Chat response completed in {chat_time:.2f}s')
        
        # Add timing info for chat responses
        timing_info = f"\n\n‚ö° *Response generated in {chat_time:.2f}s*"
        final_response = response_text + timing_info
        
        # Cache the chat response for future identical requests
        try:
            cache_data = {'response': final_response}
            cache_manager.set(cache_key, cache_data, ttl=1800)  # Cache for 30 minutes
            print(f"üíæ CACHED: Chat response for future requests")
        except Exception as e:
            print(f"‚ö†Ô∏è Chat cache storage failed: {e}")
        
        # Clean up after chat
        cleanup_cache()
        
        return final_response
        
    except Exception as e:
        print(f"Gemma 3 chat error: {e}")
        cleanup_cache()  # Clean up on error too
        return f"I apologize, but I'm experiencing technical difficulties accessing the video analysis. As an advanced AI video analysis agent, I'm designed to provide comprehensive insights about your video content. Please try asking your question again, or if the issue persists, you may need to re-analyze the video to restore full agentic capabilities." 